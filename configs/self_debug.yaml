run_mode: "self_debug"
debug:
  top_k: 1
  # output_dir: "debug_results/baseline_self_debug/plotly/"
  output_dir: "debug_results/qwen2_5_7b_coder_stage4_lr5e6_self_debug/plotly/"
paths:
  # out_folder: "eval_results/baseline_self_debug/plotly/" # for output files
  out_folder: "eval_results/qwen2_5_7b_coder_stage4_lr5e6_self_debug/plotly/" # for output files
  dataset_folder: "dataset"
  results_filename: "results.json" # json stores temporarily plotting responses. Located in the out_folder
  bench_stat_filename: "benchmark_stat.jsonl" # json stores final statistics of the benchmark results. Located in the out_folder
  # error_rate_file: "eval_results/baseline_self_debug.json"
  error_rate_file: "eval_results/qwen2_5_7b_coder_stage4_lr5e6_self_debug.json"
  # instructs_file: "instructs/instructs.json" # Optional. Default instructs exists. json stores instructs for plot generation and LLM-benchmarking.
benchmark_types: ["vis", "task"] # Options: "vis", "task", "codebert"
plotting_lib: "plotly" # "matplotlib", "seaborn", "plotly", "lets-plot" (does not work)
data_descriptor: "head" # data descriptor. Options: "pycharm", "datalore", "lida", "head", "describe", "empty"
model_plot_gen:
  names: []
  parameters: # List of additional model parameters.
    temperature: 0.0
model_judge:
  name: "openai/gpt-4o-2024-05-13"
  parameters: # List of additional model parameters.
    temperature: 0.0